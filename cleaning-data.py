import pandas as pd
from PIL import Image
import os
import glob
from datasets import Dataset, DatasetDict
import json

def create_trocr_dataset(data_path, output_dataset_path):
    """
    Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ TrOCR
    """
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª TrOCR...")
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªÙ…Ø§Ù… Ù…ØªÙ†â€ŒÙ‡Ø§
    text_files = sorted(glob.glob(os.path.join(data_path, 'fulltext', '*.txt')))
    image_files = sorted(glob.glob(os.path.join(data_path, 'images', '*.png')))
    
    print(f"ğŸ“– ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†: {len(text_files)}")
    print(f"ğŸ–¼ï¸  ØªØ¹Ø¯Ø§Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±: {len(image_files)}")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒØ³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
    samples = []
    
    for i, (text_file, image_file) in enumerate(zip(text_files, image_files)):
        try:
            # Ø®ÙˆØ§Ù†Ø¯Ù† Ù…ØªÙ†
            with open(text_file, 'r', encoding='utf-8') as f:
                text = f.read().strip()
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ ØªØµÙˆÛŒØ± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            if not os.path.exists(image_file):
                print(f"âš ï¸  ØªØµÙˆÛŒØ± {image_file} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                continue
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
            samples.append({
                'image_path': image_file,
                'text': text
            })
            
            if (i + 1) % 100 == 0:
                print(f"ğŸ“¦ {i + 1} Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯...")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ù…ÙˆÙ†Ù‡ {i}: {e}")
    
    print(f"âœ… {len(samples)} Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯")
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ train/validation/test
    train_size = int(0.8 * len(samples))
    val_size = int(0.1 * len(samples))
    
    train_samples = samples[:train_size]
    val_samples = samples[train_size:train_size + val_size]
    test_samples = samples[train_size + val_size:]
    
    print(f"\nğŸ“Š ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
    print(f"   ğŸ‹ï¸  Ø¢Ù…ÙˆØ²Ø´: {len(train_samples)} Ù†Ù…ÙˆÙ†Ù‡")
    print(f"   ğŸ“Š Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ: {len(val_samples)} Ù†Ù…ÙˆÙ†Ù‡")
    print(f"   ğŸ§ª ØªØ³Øª: {len(test_samples)} Ù†Ù…ÙˆÙ†Ù‡")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª Hugging Face
    def create_hf_dataset(samples_list):
        """ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø³Øª Hugging Face"""
        dataset = Dataset.from_dict({
            'image': [sample['image_path'] for sample in samples_list],
            'text': [sample['text'] for sample in samples_list]
        })
        return dataset
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    train_dataset = create_hf_dataset(train_samples)
    val_dataset = create_hf_dataset(val_samples)
    test_dataset = create_hf_dataset(test_samples)
    
    # Ø§ÛŒØ¬Ø§Ø¯ DatasetDict
    dataset_dict = DatasetDict({
        'train': train_dataset,
        'validation': val_dataset,
        'test': test_dataset
    })
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª
    dataset_dict.save_to_disk(output_dataset_path)
    print(f"ğŸ’¾ Ø¯ÛŒØªØ§Ø³Øª Ø¯Ø± '{output_dataset_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    return dataset_dict

def test_dataset_loading(dataset_path):
    """
    ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª
    """
    print("\nğŸ” ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª...")
    
    from datasets import load_from_disk
    
    try:
        dataset = load_from_disk(dataset_path)
        
        print("âœ… Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        print(f"ğŸ“ Ø³Ø§Ø®ØªØ§Ø±: {dataset}")
        
        # Ù†Ù…Ø§ÛŒØ´ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡
        print(f"\nğŸ‘€ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´:")
        sample = dataset['train'][0]
        print(f"   ğŸ“„ Ù…ØªÙ†: {sample['text'][:100]}...")
        print(f"   ğŸ–¼ï¸  Ù…Ø³ÛŒØ± ØªØµÙˆÛŒØ±: {sample['image']}")
        
        # ØªØ³Øª Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±
        try:
            img = Image.open(sample['image'])
            print(f"   ğŸ“ Ø§Ø¨Ø¹Ø§Ø¯ ØªØµÙˆÛŒØ±: {img.size}")
        except Exception as e:
            print(f"   âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±: {e}")
            
        return dataset
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø³Øª: {e}")
        return None

# Ù…Ø³ÛŒØ±Ù‡Ø§
data_path = "/home/f_allahmoradi/Desktop/TROCR/selected_1000_data"
output_dataset_path = "/home/f_allahmoradi/Desktop/TROCR/trocr_dataset"

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª
print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§Ø³Øª...")
dataset = create_trocr_dataset(data_path, output_dataset_path)

# ØªØ³Øª Ø¯ÛŒØªØ§Ø³Øª
if dataset:
    test_dataset_loading(output_dataset_path)
